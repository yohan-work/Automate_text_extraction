#!/usr/bin/env python3
"""
ë°°ì¹˜ URL ì²˜ë¦¬ ë„êµ¬
ì—¬ëŸ¬ê°œì˜ URLì„ í•œ ë²ˆì— ì²˜ë¦¬í•˜ì—¬ ê°ê° ë³„ë„ì˜ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥í•˜ê±°ë‚˜ í•˜ë‚˜ì˜ íŒŒì¼ì— í†µí•©
"""

import os
import csv
import json
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from cli_extractor import CLIWebTextExtractor

class BatchProcessor:
    def __init__(self, max_workers=3):
        self.extractor = CLIWebTextExtractor()
        self.max_workers = max_workers
        self.results = []
        self.lock = threading.Lock()
    
    def process_urls_from_file(self, input_file, output_dir="output", languages=['en', 'zh-cn', 'vi']):
        """íŒŒì¼ì—ì„œ URL ëª©ë¡ì„ ì½ì–´ ë°°ì¹˜ ì²˜ë¦¬"""
        urls = self.read_urls_from_file(input_file)
        
        if not urls:
            print("URLì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        return self.process_url_list(urls, output_dir, languages)
    
    def read_urls_from_file(self, file_path):
        """íŒŒì¼ì—ì„œ URL ëª©ë¡ ì½ê¸° (í…ìŠ¤íŠ¸, CSV, JSON ì§€ì›)"""
        urls = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                file_ext = os.path.splitext(file_path)[1].lower()
                
                if file_ext == '.json':
                    data = json.load(f)
                    if isinstance(data, list):
                        urls = [item if isinstance(item, str) else item.get('url', '') for item in data]
                    elif isinstance(data, dict) and 'urls' in data:
                        urls = data['urls']
                
                elif file_ext == '.csv':
                    reader = csv.reader(f)
                    for row in reader:
                        if row and row[0].strip():  # ì²« ë²ˆì§¸ ì—´ì´ URLì´ë¼ê³  ê°€ì •
                            urls.append(row[0].strip())
                
                else:  # ì¼ë°˜ í…ìŠ¤íŠ¸ íŒŒì¼
                    for line in f:
                        line = line.strip()
                        if line and not line.startswith('#'):  # ì£¼ì„ ì œì™¸
                            urls.append(line)
        
        except Exception as e:
            print(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}")
            return []
        
        # URL í˜•ì‹ ê²€ì¦ ë° ì •ë¦¬
        valid_urls = []
        for url in urls:
            if url:
                if not url.startswith(('http://', 'https://')):
                    url = 'https://' + url
                valid_urls.append(url)
        
        return valid_urls
    
    def process_url_list(self, urls, output_dir="output", languages=['en', 'zh-cn', 'vi']):
        """URL ëª©ë¡ ë°°ì¹˜ ì²˜ë¦¬"""
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        print(f"ì´ {len(urls)}ê°œì˜ URLì„ ì²˜ë¦¬í•©ë‹ˆë‹¤...")
        print(f"ë²ˆì—­ ì–¸ì–´: {', '.join(languages)}")
        print(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
        print(f"ìµœëŒ€ ë™ì‹œ ì²˜ë¦¬: {self.max_workers}ê°œ")
        print("-" * 50)
        
        successful = 0
        failed = 0
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # ì‘ì—… ì œì¶œ
            future_to_url = {}
            for i, url in enumerate(urls, 1):
                output_file = os.path.join(output_dir, f"ì›¹í…ìŠ¤íŠ¸_{i:03d}_{self.url_to_filename(url)}.xlsx")
                future = executor.submit(self.process_single_url, url, output_file, languages, i, len(urls))
                future_to_url[future] = url
            
            # ê²°ê³¼ ìˆ˜ì§‘
            for future in as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    result = future.result()
                    if result:
                        successful += 1
                    else:
                        failed += 1
                except Exception as e:
                    print(f"âŒ {url} - ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
                    failed += 1
        
        print("-" * 50)
        print(f"ì²˜ë¦¬ ì™„ë£Œ! ì„±ê³µ: {successful}ê°œ, ì‹¤íŒ¨: {failed}ê°œ")
        
        # ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±
        self.generate_report(output_dir, successful, failed)
        
        return successful > 0
    
    def process_single_url(self, url, output_file, languages, current, total):
        """ë‹¨ì¼ URL ì²˜ë¦¬"""
        try:
            print(f"[{current}/{total}] ì²˜ë¦¬ ì‹œì‘: {url}")
            
            text_elements = self.extractor.extract_text_from_url(url, verbose=False)
            
            if not text_elements:
                print(f"âŒ [{current}/{total}] í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {url}")
                return False
            
            success = self.extractor.create_excel_file(text_elements, output_file, languages, verbose=False)
            
            if success:
                print(f"âœ… [{current}/{total}] ì™„ë£Œ: {os.path.basename(output_file)}")
                
                # ê²°ê³¼ ê¸°ë¡
                with self.lock:
                    self.results.append({
                        'url': url,
                        'output_file': output_file,
                        'text_count': len(text_elements),
                        'status': 'success',
                        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    })
                
                return True
            else:
                print(f"âŒ [{current}/{total}] ì—‘ì…€ ìƒì„± ì‹¤íŒ¨: {url}")
                return False
                
        except Exception as e:
            print(f"âŒ [{current}/{total}] ì˜¤ë¥˜: {url} - {str(e)}")
            return False
    
    def url_to_filename(self, url):
        """URLì„ íŒŒì¼ëª…ìœ¼ë¡œ ë³€í™˜"""
        # URLì—ì„œ ë„ë©”ì¸ ì¶”ì¶œ
        from urllib.parse import urlparse
        parsed = urlparse(url)
        domain = parsed.netloc.replace('www.', '')
        
        # íŒŒì¼ëª…ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ë¬¸ì ì œê±°
        safe_chars = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789-_"
        filename = ''.join(c if c in safe_chars else '_' for c in domain)
        
        return filename[:30]  # ë„ˆë¬´ ê¸´ íŒŒì¼ëª… ë°©ì§€
    
    def generate_report(self, output_dir, successful, failed):
        """ì²˜ë¦¬ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        report_file = os.path.join(output_dir, f"ì²˜ë¦¬ê²°ê³¼_ë¦¬í¬íŠ¸_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")
        
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write("ì›¹ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ ë¦¬í¬íŠ¸\n")
                f.write("=" * 50 + "\n")
                f.write(f"ì²˜ë¦¬ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"ì„±ê³µ: {successful}ê°œ\n")
                f.write(f"ì‹¤íŒ¨: {failed}ê°œ\n")
                f.write(f"ì´ ì²˜ë¦¬: {successful + failed}ê°œ\n\n")
                
                if self.results:
                    f.write("ìƒì„¸ ê²°ê³¼:\n")
                    f.write("-" * 30 + "\n")
                    for result in self.results:
                        f.write(f"URL: {result['url']}\n")
                        f.write(f"íŒŒì¼: {os.path.basename(result['output_file'])}\n")
                        f.write(f"í…ìŠ¤íŠ¸ ìˆ˜: {result['text_count']}ê°œ\n")
                        f.write(f"ìƒíƒœ: {result['status']}\n")
                        f.write(f"ì‹œê°„: {result['timestamp']}\n")
                        f.write("-" * 30 + "\n")
            
            print(f"ğŸ“Š ì²˜ë¦¬ ë¦¬í¬íŠ¸ ìƒì„±: {report_file}")
            
        except Exception as e:
            print(f"ë¦¬í¬íŠ¸ ìƒì„± ì˜¤ë¥˜: {str(e)}")

def create_sample_url_file():
    """ìƒ˜í”Œ URL íŒŒì¼ ìƒì„±"""
    sample_urls = [
        "https://www.lgdisplay.com/kor/sustainability/esg-strategy",
        "https://news.naver.com",
        "https://www.korea.kr",
        "# ì´ê²ƒì€ ì£¼ì„ì…ë‹ˆë‹¤",
        "https://example.com"
    ]
    
    # í…ìŠ¤íŠ¸ íŒŒì¼
    with open("sample_urls.txt", "w", encoding="utf-8") as f:
        for url in sample_urls:
            f.write(url + "\n")
    
    # JSON íŒŒì¼
    json_data = {
        "urls": [url for url in sample_urls if not url.startswith("#")],
        "description": "ì›¹ í…ìŠ¤íŠ¸ ì¶”ì¶œì„ ìœ„í•œ ìƒ˜í”Œ URL ëª©ë¡"
    }
    
    with open("sample_urls.json", "w", encoding="utf-8") as f:
        json.dump(json_data, f, ensure_ascii=False, indent=2)
    
    # CSV íŒŒì¼
    with open("sample_urls.csv", "w", encoding="utf-8", newline='') as f:
        writer = csv.writer(f)
        writer.writerow(["URL", "ì„¤ëª…"])
        writer.writerow(["https://www.lgdisplay.com/kor/sustainability/esg-strategy", "LGë””ìŠ¤í”Œë ˆì´ ESG"])
        writer.writerow(["https://news.naver.com", "ë„¤ì´ë²„ ë‰´ìŠ¤"])
        writer.writerow(["https://www.korea.kr", "ëŒ€í•œë¯¼êµ­ ì •ë¶€"])
    
    print("ğŸ“ ìƒ˜í”Œ íŒŒì¼ ìƒì„± ì™„ë£Œ:")
    print("  - sample_urls.txt")
    print("  - sample_urls.json") 
    print("  - sample_urls.csv")

def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description="ë°°ì¹˜ ì›¹ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ë²ˆì—­ ë„êµ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  python batch_processor.py urls.txt
  python batch_processor.py urls.json -o results -l en zh-cn
  python batch_processor.py urls.csv --workers 5
  python batch_processor.py --create-sample
        """
    )
    
    parser.add_argument('input_file', nargs='?', help='URLì´ í¬í•¨ëœ íŒŒì¼ (txt, json, csv)')
    parser.add_argument('-o', '--output-dir', default='output', 
                       help='ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: output)')
    parser.add_argument('-l', '--languages', nargs='+', 
                       default=['en', 'zh-cn', 'vi'],
                       help='ë²ˆì—­í•  ì–¸ì–´ ì½”ë“œ (ê¸°ë³¸ê°’: en zh-cn vi)')
    parser.add_argument('-w', '--workers', type=int, default=3,
                       help='ë™ì‹œ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ìˆ˜ (ê¸°ë³¸ê°’: 3)')
    parser.add_argument('--create-sample', action='store_true',
                       help='ìƒ˜í”Œ URL íŒŒì¼ë“¤ ìƒì„±')
    
    args = parser.parse_args()
    
    if args.create_sample:
        create_sample_url_file()
        return
    
    if not args.input_file:
        parser.print_help()
        return
    
    if not os.path.exists(args.input_file):
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.input_file}")
        return
    
    processor = BatchProcessor(max_workers=args.workers)
    
    try:
        success = processor.process_urls_from_file(
            args.input_file,
            args.output_dir,
            args.languages
        )
        
        if success:
            print("ğŸ‰ ë°°ì¹˜ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        else:
            print("âŒ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì‘ì—…ì´ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

if __name__ == "__main__":
    main() 